# T2I
- KV Inversion: KV Embeddings Learning for Text-Conditioned Real Image Action Editing[[paper]](https://browse.arxiv.org/pdf/2309.16608v1.pdf)
- PixArt-$Î±$: Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis[[paper]](https://browse.arxiv.org/pdf/2310.00426v1.pdf)[[project]](https://pixart-alpha.github.io/)
- EditVal: Benchmarking Diffusion Based Text-Guided Image Editing Methods[[paper]](http://arxiv.org/pdf/2310.02426v1)[[project]](https://deep-ml-research.github.io/editval/)
- LATENT CONSISTENCY MODELS:SYNTHESIZING HIGH-RESOLUTION IMAGES WITH FEW-STEP INFERENCE[[paper]](https://arxiv.org/pdf/2310.04378v1.pdf)[[project]](https://latent-consistency-models.github.io/)
- EasyPhoto: Your Personal AI Photo Generator[[paper]](https://arxiv.org/pdf/2310.04672v1.pdf)[[project]](https://github.com/aigc-apps/sd-webui-EasyPhoto)
-  Advancing Pose-Guided Image Synthesis with Progressive Conditional Diffusion Models[[paper]](http://arxiv.org/pdf/2310.06313v1)[[project]](https://github.com/muzishen/PCDMs)
- MULTI-CONCEPT T2I-ZERO: TWEAKING ONLY THE TEXT EMBEDDINGS AND NOTHING ELSE[[paper]](https://arxiv.org/pdf/2310.07419v1.pdf)[[project](https://multi-concept-t2i-zero.github.io/)]
- GENEVAL: An Object-Focused Framework for Evaluating Text-to-Image Alignment [[paper]](http://arxiv.org/pdf/2310.11513v1)
- Learning to Follow Object-Centric Image Editing Instructions Faithfully[[paper]](https://arxiv.org/pdf/2310.19145v1.pdf)[[project]](https://github.com/tuhinjubcse/FaithfulEdits_EMNLP2023)
# T2V
- CCEdit: Creative and Controllable Video Editing via Diffusion Models[[paper]](https://browse.arxiv.org/pdf/2309.16496v1.pdf)
- LLM-grounded Video Diffusion Models[[paper]](https://browse.arxiv.org/pdf/2309.17444v1.pdf)
- Ground-A-Video: Zero-shot Grounded Video Editing using Text-to-image Diffusion Models[[paper]](https://browse.arxiv.org/pdf/2310.01107v1.pdf)[[project]](https://ground-a-video.github.io/)
- FLATTEN: OPTICAL FLOW-GUIDED ATTENTION FOR CONSISTENT TEXT-TO-VIDEO EDITING[[paper]](https://arxiv.org/pdf/2310.05922v1.pdf)[[project]](https://flatten-video-editing.github.io/)
- ConditionVideo: Training-Free Condition-Guided Text-to-Video Generation[[paper]](https://arxiv.org/pdf/2310.07697v1.pdf)[[project]](https://pengbo807.github.io/conditionvideo-website/)
- A Survey on Video Diffusion Models [[paper]](https://arxiv.org/pdf/2310.10647v1.pdf)[[project]](https://github.com/ChenHsing/Awesome-Video-Diffusion-Models)
- LOVECon: Text-driven Training-Free Long Video Editing with ControlNet[[paper]](http://arxiv.org/pdf/2310.09711v1)[[project]](https://github.com/zhijie-group/LOVECon)
- FreeNoise: Tuning-Free Longer Video Diffusion Via Noise Rescheduling[[project]](http://haonanqiu.com/projects/FreeNoise.html)[[paper]](https://arxiv.org/pdf/2310.15169v1.pdf)
- CVPR 2023 Text Guided Video Editing Competition[[paper]](https://arxiv.org/pdf/2310.16003v1.pdf)[[project]](https://sites.google.com/view/loveucvpr23/track4)
- Fuse Your Latents: Video Editing with Multi-source Latent Diffusion Models[[paper]](http://arxiv.org/pdf/2310.16400v1)
- VideoCrafter1: Open Diffusion Models for High-Quality Video Generation[[paper]](https://arxiv.org/pdf/2310.19512v1.pdf)[[project]](https://ailab-cvc.github.io/videocrafter/)